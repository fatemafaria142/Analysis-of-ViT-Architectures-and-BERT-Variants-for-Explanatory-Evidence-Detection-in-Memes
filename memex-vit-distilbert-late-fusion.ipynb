{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7127405,"sourceType":"datasetVersion","datasetId":4104677}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nltk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-06T04:43:59.795567Z","iopub.execute_input":"2023-12-06T04:43:59.795975Z","iopub.status.idle":"2023-12-06T04:44:12.287938Z","shell.execute_reply.started":"2023-12-06T04:43:59.795858Z","shell.execute_reply":"2023-12-06T04:44:12.286782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport os","metadata":{"execution":{"iopub.status.busy":"2023-12-06T04:44:20.045271Z","iopub.execute_input":"2023-12-06T04:44:20.045743Z","iopub.status.idle":"2023-12-06T04:44:20.050445Z","shell.execute_reply.started":"2023-12-06T04:44:20.045714Z","shell.execute_reply":"2023-12-06T04:44:20.049498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the CSV file into a Pandas DataFrame\ndata = pd.read_csv('/kaggle/input/memexdata/images_info.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-06T04:44:22.861702Z","iopub.execute_input":"2023-12-06T04:44:22.862773Z","iopub.status.idle":"2023-12-06T04:44:22.915269Z","shell.execute_reply.started":"2023-12-06T04:44:22.862734Z","shell.execute_reply":"2023-12-06T04:44:22.914392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Read the CSV file into a Pandas DataFrame\ndata = pd.read_csv('/kaggle/input/memexdata/images_info.csv')\n\n# Splitting the 'image_path' column to extract filenames\ndata['image_path'] = data['image_path'].apply(lambda x: x.split('/')[-1].split('.')[0])  # Extracts the filename without extension\ndata['sentences'] = data['sentences']  # Example assignment, you might have different data to put here\ndata['Meme_Caption'] = data['Meme_Caption']  # Example assignment, you might have different data to put here\ndata['Relevance_Score'] = data['Relevance_Score']  # Example assignment, you might have different data to put here\n\n# Save the modified DataFrame to a new CSV file\ndata.to_csv('/kaggle/working/modified_images_info.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-06T04:44:25.308222Z","iopub.execute_input":"2023-12-06T04:44:25.308580Z","iopub.status.idle":"2023-12-06T04:44:25.342725Z","shell.execute_reply.started":"2023-12-06T04:44:25.308550Z","shell.execute_reply":"2023-12-06T04:44:25.341869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the CSV file into a Pandas DataFrame\ndata = pd.read_csv('/kaggle/working/modified_images_info.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-06T04:44:27.475261Z","iopub.execute_input":"2023-12-06T04:44:27.476061Z","iopub.status.idle":"2023-12-06T04:44:27.498024Z","shell.execute_reply.started":"2023-12-06T04:44:27.476028Z","shell.execute_reply":"2023-12-06T04:44:27.497222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# Path to your image folder\nimage_folder_path = '/kaggle/input/memexdata/Meme/Meme/images'\n\n# Load the CSV file into a Pandas DataFrame\ndata = pd.read_csv('/kaggle/working/modified_images_info.csv')\n\n# Create lists to store image paths and information\nimage_paths = []\nsentences_list = []\nevidence_list = []\nmeme_captions = []\nrelevance_scores = []\n\n# Iterate through the CSV data and map images from the folder\nfor index, row in data.iterrows():\n    image_name = str(row['image_path']) + '.jpg'  # Assuming the image paths in CSV are numbers without extension\n    image_path = os.path.join(image_folder_path, image_name)\n    \n    # Check if the image exists in the folder\n    if os.path.exists(image_path):\n        image_paths.append(image_path)\n        sentences_list.append(row['sentences'])\n        evidence_list.append(row['evidence'])\n        meme_captions.append(row['Meme_Caption'])\n        relevance_scores.append(row['Relevance_Score'])\n\n# Create a DataFrame from the mapped information\nimage_info = pd.DataFrame({\n    'image_path': image_paths,\n    'sentences': sentences_list,\n    'evidence': evidence_list,\n    'Meme_Caption': meme_captions,\n    'Relevance_Score': relevance_scores\n})\n\n# Save the DataFrame to a CSV file\nimage_info.to_csv('/kaggle/working/images_info_mapped.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-06T04:44:29.596247Z","iopub.execute_input":"2023-12-06T04:44:29.597131Z","iopub.status.idle":"2023-12-06T04:44:33.662412Z","shell.execute_reply.started":"2023-12-06T04:44:29.597087Z","shell.execute_reply":"2023-12-06T04:44:33.661431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the CSV file into a Pandas DataFrame\ndata = pd.read_csv('/kaggle/working/images_info_mapped.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-06T04:44:36.724971Z","iopub.execute_input":"2023-12-06T04:44:36.725297Z","iopub.status.idle":"2023-12-06T04:44:36.747154Z","shell.execute_reply.started":"2023-12-06T04:44:36.725272Z","shell.execute_reply":"2023-12-06T04:44:36.746306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Splitting the data into train and temp sets (80% train, 20% temp)\ntrain_df, temp_df = train_test_split(data, test_size=0.2, random_state=42)\n\n# Splitting the temp data into test and validation sets (50% test, 50% validation from the temp data)\ntest_df, validation_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\n# Display the sizes of each split\nprint(f\"Train dataset size: {len(train_df)}\")\nprint(f\"Test dataset size: {len(test_df)}\")\nprint(f\"Validation dataset size: {len(validation_df)}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T04:44:41.189512Z","iopub.execute_input":"2023-12-06T04:44:41.190324Z","iopub.status.idle":"2023-12-06T04:44:41.396514Z","shell.execute_reply.started":"2023-12-06T04:44:41.190279Z","shell.execute_reply":"2023-12-06T04:44:41.395431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom transformers import BertTokenizer, BertModel, AdamW\nimport torchvision.models as models\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-12-06T04:44:43.453285Z","iopub.execute_input":"2023-12-06T04:44:43.453928Z","iopub.status.idle":"2023-12-06T04:44:48.631705Z","shell.execute_reply.started":"2023-12-06T04:44:43.453895Z","shell.execute_reply":"2023-12-06T04:44:48.630942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"execution":{"iopub.status.busy":"2023-12-06T04:55:05.352436Z","iopub.execute_input":"2023-12-06T04:55:05.353131Z","iopub.status.idle":"2023-12-06T04:55:26.051519Z","shell.execute_reply.started":"2023-12-06T04:55:05.353095Z","shell.execute_reply":"2023-12-06T04:55:26.050505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\nfrom transformers import AutoImageProcessor, ViTModel\n\nimage_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\nmodel = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T05:09:50.120165Z","iopub.execute_input":"2023-12-06T05:09:50.120528Z","iopub.status.idle":"2023-12-06T05:09:51.470862Z","shell.execute_reply.started":"2023-12-06T05:09:50.120501Z","shell.execute_reply":"2023-12-06T05:09:51.469815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, DistilBertModel, AdamW\n\n# Initialize BERT tokenizer and model\nbert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\nbert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T05:09:53.382927Z","iopub.execute_input":"2023-12-06T05:09:53.383303Z","iopub.status.idle":"2023-12-06T05:09:54.178259Z","shell.execute_reply.started":"2023-12-06T05:09:53.383269Z","shell.execute_reply":"2023-12-06T05:09:54.177117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T05:09:56.390723Z","iopub.execute_input":"2023-12-06T05:09:56.391475Z","iopub.status.idle":"2023-12-06T05:09:56.396719Z","shell.execute_reply.started":"2023-12-06T05:09:56.391441Z","shell.execute_reply":"2023-12-06T05:09:56.395787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T05:09:57.910255Z","iopub.execute_input":"2023-12-06T05:09:57.910622Z","iopub.status.idle":"2023-12-06T05:09:58.011633Z","shell.execute_reply.started":"2023-12-06T05:09:57.910590Z","shell.execute_reply":"2023-12-06T05:09:58.010752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T05:10:01.103349Z","iopub.execute_input":"2023-12-06T05:10:01.103694Z","iopub.status.idle":"2023-12-06T05:10:01.177323Z","shell.execute_reply.started":"2023-12-06T05:10:01.103667Z","shell.execute_reply":"2023-12-06T05:10:01.176457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport torchvision.models as models","metadata":{"execution":{"iopub.status.busy":"2023-12-06T05:10:05.785700Z","iopub.execute_input":"2023-12-06T05:10:05.786440Z","iopub.status.idle":"2023-12-06T05:10:05.791019Z","shell.execute_reply.started":"2023-12-06T05:10:05.786409Z","shell.execute_reply":"2023-12-06T05:10:05.790097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\n\nmax_seq_length = 512  # Set your desired maximum sequence length for BERT\n\n# Define the pre-processing transformations for images\ntransform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nclass MyMultimodalDataset(Dataset):\n    def __init__(self, data, transform=None, tokenizer=None, max_seq_length=128):\n        self.data = data\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image_path = self.data.iloc[idx]['image_path']\n        image = Image.open(image_path).convert('RGB')\n        if self.transform is not None:\n            image = self.transform(image)\n\n        sentences = self.data.iloc[idx]['sentences']\n        evidence = self.data.iloc[idx]['evidence']\n        meme_caption = self.data.iloc[idx]['Meme_Caption']\n\n        # Concatenate all text inputs into a single string\n        combined_text = f\"{sentences} {evidence} {meme_caption}\"\n\n        inputs = self.tokenizer(combined_text, padding='max_length', truncation=True, max_length=self.max_seq_length, return_tensors='pt')\n        input_ids = inputs['input_ids']\n        attention_mask = inputs['attention_mask']\n\n        label = self.data.iloc[idx]['Relevance_Score']\n\n        return image, input_ids, attention_mask, label\n","metadata":{"execution":{"iopub.status.busy":"2023-12-06T05:10:07.802090Z","iopub.execute_input":"2023-12-06T05:10:07.802442Z","iopub.status.idle":"2023-12-06T05:10:07.814266Z","shell.execute_reply.started":"2023-12-06T05:10:07.802413Z","shell.execute_reply":"2023-12-06T05:10:07.813407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create custom datasets with MyMultimodalDataset\ntrain_dataset = MyMultimodalDataset(train_df, transform=transform, tokenizer=bert_tokenizer, max_seq_length=max_seq_length)\ntest_dataset = MyMultimodalDataset(test_df, transform=transform, tokenizer=bert_tokenizer, max_seq_length=max_seq_length)\nval_dataset = MyMultimodalDataset(validation_df, transform=transform, tokenizer=bert_tokenizer, max_seq_length=max_seq_length)\n\n# Define data loaders\nbatch_size = 5  # Set your desired batch size\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-06T05:10:10.960539Z","iopub.execute_input":"2023-12-06T05:10:10.960928Z","iopub.status.idle":"2023-12-06T05:10:10.974434Z","shell.execute_reply.started":"2023-12-06T05:10:10.960872Z","shell.execute_reply":"2023-12-06T05:10:10.973425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport time\nfrom torch.optim import AdamW\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report","metadata":{"execution":{"iopub.status.busy":"2023-12-06T05:10:14.185753Z","iopub.execute_input":"2023-12-06T05:10:14.186414Z","iopub.status.idle":"2023-12-06T05:10:14.191573Z","shell.execute_reply.started":"2023-12-06T05:10:14.186381Z","shell.execute_reply":"2023-12-06T05:10:14.190531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define optimizer and loss function\noptimizer = AdamW(list(model.parameters()) + list(bert_model.parameters()), lr=2e-5)\ncriterion = torch.nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-12-06T05:10:16.866104Z","iopub.execute_input":"2023-12-06T05:10:16.866810Z","iopub.status.idle":"2023-12-06T05:10:16.873337Z","shell.execute_reply.started":"2023-12-06T05:10:16.866780Z","shell.execute_reply":"2023-12-06T05:10:16.872432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\nfrom transformers import AutoImageProcessor, ViTModel, AutoTokenizer, DistilBertModel, AdamW\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nimport time\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nnum_epochs = 1\ntrain_losses = []\nval_losses = []\n\n# Training time\nstart_train_time = time.time()\n\n\nfor epoch in range(num_epochs):\n    running_train_loss = 0.0\n    \n    model.train()\n    bert_model.train()\n\n    for images, texts, attention_masks, labels in train_loader:\n        images = images.to(device)\n        labels = (labels - 1).to(device)\n\n        input_ids = texts.squeeze(1).to(device)\n        attention_mask = attention_masks.squeeze(1).to(device)\n\n        optimizer.zero_grad()\n\n        with torch.no_grad():\n            outputs_image = model(pixel_values=images)\n        img_hidden_states = outputs_image.last_hidden_state\n        img_feats = img_hidden_states[:, 0, :]\n\n        outputs_text = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n        text_hidden_states = outputs_text.last_hidden_state\n        text_feats = text_hidden_states[:, 0, :]\n\n        combined_feats = torch.cat((img_feats, text_feats), dim=1)\n\n        regressor = torch.nn.Sequential(\n            torch.nn.Linear(combined_feats.shape[1], 512).to(device),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(512, 5).to(device),  # Change output size to 5 for 5 labels\n        )\n\n        predictions = regressor(combined_feats)\n        loss = criterion(predictions, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        running_train_loss += loss.item()\n\n    epoch_train_loss = running_train_loss / len(train_loader)\n    train_losses.append(epoch_train_loss)\n\n    model.eval()\n    bert_model.eval()\n\n    running_val_loss = 0.0\n\n    with torch.no_grad():\n        for val_images, val_texts, val_attention_masks, val_labels in val_loader:\n            val_images = val_images.to(device)\n            val_labels = (val_labels - 1).to(device)\n\n            val_input_ids = val_texts.squeeze(1).to(device)\n            val_attention_mask = val_attention_masks.squeeze(1).to(device)\n\n            outputs_image = model(pixel_values=val_images)\n            val_img_hidden_states = outputs_image.last_hidden_state\n            val_img_feats = val_img_hidden_states[:, 0, :]\n\n            outputs_text = bert_model(input_ids=val_input_ids, attention_mask=val_attention_mask)\n            val_text_hidden_states = outputs_text.last_hidden_state\n            val_text_feats = val_text_hidden_states[:, 0, :]\n\n            val_combined_feats = torch.cat((val_img_feats, val_text_feats), dim=1)\n\n            val_predictions = regressor(val_combined_feats)\n            val_loss = criterion(val_predictions, val_labels)\n\n            running_val_loss += val_loss.item()\n\n    epoch_val_loss = running_val_loss / len(val_loader)\n    val_losses.append(epoch_val_loss)\n\n    print(f\"Epoch [{epoch + 1}/{num_epochs}] - \"\n          f\"Train Loss: {epoch_train_loss:.4f}, \"\n          f\"Val Loss: {epoch_val_loss:.4f}\")\n\n    \nend_time = time.time()\nexecution_time = end_time - start_time\nprint(f\"Total execution time: {execution_time:.2f} seconds\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-06T05:31:06.409649Z","iopub.execute_input":"2023-12-06T05:31:06.410576Z","iopub.status.idle":"2023-12-06T05:31:16.251839Z","shell.execute_reply.started":"2023-12-06T05:31:06.410536Z","shell.execute_reply":"2023-12-06T05:31:16.250842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nimport time\n\n# Set models to evaluation mode\nmodel.eval()\nbert_model.eval()\n\n# Prepare lists to store predicted and true labels\npredicted_labels = []\ntrue_labels = []\n\n# Set your device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Test loop\nstart_time = time.time()\n\nwith torch.no_grad():\n    for test_images, test_texts, test_attention_masks, test_labels in tqdm(test_loader, desc='Testing', leave=False):\n        test_images = test_images.to(device)\n        test_labels = (test_labels - 1).to(device)  # Convert labels from 1-5 to 0-4\n        test_texts = test_texts.to(device)\n        test_attention_masks = test_attention_masks.to(device)\n\n        optimizer.zero_grad()\n\n        # Extract features simultaneously\n        test_img_feats = model(test_images)\n        test_texts = test_texts.squeeze(1)\n        test_attention_masks = test_attention_masks.squeeze(1)\n        test_outputs = bert_model(input_ids=test_texts, attention_mask=test_attention_masks)\n        \n        # Modify the shape to match for concatenation\n        test_img_feats = test_img_feats.pooler_output.view(test_img_feats.pooler_output.size(0), -1)\n        test_text_feats = test_outputs.last_hidden_state[:, 0, :]\n\n        # Combine features early\n        combined_feats = torch.cat((test_img_feats, test_text_feats), dim=1)\n\n        # Classify combined features\n        combined_classifier = torch.nn.Sequential(\n            torch.nn.Linear(combined_feats.shape[1], 512).to(device),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(512, 5).to(device),  # Change the output size to 5 for 5 labels\n        )\n\n        combined_logits = combined_classifier(combined_feats)\n        test_predictions = torch.nn.functional.softmax(combined_logits, dim=1)\n        predicted_classes = torch.argmax(test_predictions, dim=1) + 1  # Revert back to labels from 1-5, Add 1 here \n\n        predicted_labels.extend(predicted_classes.cpu().numpy())\n        true_labels.extend(test_labels.cpu().numpy().tolist())\n\nend_time = time.time()\nexecution_time = end_time - start_time\n\n# Print or use the predicted labels and true labels as needed\nprint(\"Predicted Labels:\", predicted_labels)\nprint(\"True Labels:\", true_labels)\nprint(f\"Total execution time for testing: {execution_time:.2f} seconds\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-06T05:34:26.672294Z","iopub.execute_input":"2023-12-06T05:34:26.672680Z","iopub.status.idle":"2023-12-06T05:34:27.598277Z","shell.execute_reply.started":"2023-12-06T05:34:26.672651Z","shell.execute_reply":"2023-12-06T05:34:27.597429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_labels ","metadata":{"execution":{"iopub.status.busy":"2023-12-06T05:34:31.042332Z","iopub.execute_input":"2023-12-06T05:34:31.043084Z","iopub.status.idle":"2023-12-06T05:34:31.049571Z","shell.execute_reply.started":"2023-12-06T05:34:31.043032Z","shell.execute_reply":"2023-12-06T05:34:31.048579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_labels ","metadata":{"execution":{"iopub.status.busy":"2023-12-06T05:34:32.588677Z","iopub.execute_input":"2023-12-06T05:34:32.589438Z","iopub.status.idle":"2023-12-06T05:34:32.597364Z","shell.execute_reply.started":"2023-12-06T05:34:32.589390Z","shell.execute_reply":"2023-12-06T05:34:32.596368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, cohen_kappa_score, mean_squared_error, classification_report, roc_auc_score, hamming_loss\n\n\n# Calculate accuracy\naccuracy = accuracy_score(true_labels, predicted_labels)\n\n# Calculate precision, recall, F1-score overall (macro average)\nprecision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='macro')\n\n# Calculate confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels)\n\n# Calculate Mean Squared Error\nmse = mean_squared_error(true_labels, predicted_labels)\n\n# Print overall calculated metrics\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Precision (macro): {precision}\")\nprint(f\"Recall (macro): {recall}\")\nprint(f\"F1-Score (macro): {f1_score}\")\nprint(f\"Mean Squared Error: {mse}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T05:34:35.091276Z","iopub.execute_input":"2023-12-06T05:34:35.091695Z","iopub.status.idle":"2023-12-06T05:34:35.109143Z","shell.execute_reply.started":"2023-12-06T05:34:35.091661Z","shell.execute_reply":"2023-12-06T05:34:35.108122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate classification report\nclassification_rep = classification_report(true_labels, predicted_labels)\nprint(f\"Classification Report:\\n{classification_rep}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T05:34:37.580480Z","iopub.execute_input":"2023-12-06T05:34:37.580878Z","iopub.status.idle":"2023-12-06T05:34:37.596921Z","shell.execute_reply.started":"2023-12-06T05:34:37.580848Z","shell.execute_reply":"2023-12-06T05:34:37.595700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\n# Assuming conf_matrix is your confusion matrix with shape (n_classes, n_classes)\nconf_matrix = confusion_matrix(true_labels, predicted_labels)\n\n# Set font properties\nfont = {'family': 'serif', 'weight': 'bold', 'size': 12}\nfont2 = {'family': 'serif', 'weight': 'bold', 'size': 14}\n\n# Calculate accuracy\naccuracy = accuracy_score(true_labels, predicted_labels)*100\n\n# Create a heatmap using the 'Paired' color palette without color bar\nplt.figure(figsize=(5, 4))\nsns.heatmap(conf_matrix, annot=True, cmap=\"crest\", fmt='g', cbar=False)\n\nplt.xlabel('Predicted Labels', labelpad=12, fontdict=font)\nplt.ylabel('True Labels', labelpad=12, fontdict=font)\nplt.title(f'Relevance Score\\nAccuracy: {accuracy:.2f}%', pad=12, fontdict=font2)\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-06T05:34:40.580040Z","iopub.execute_input":"2023-12-06T05:34:40.580847Z","iopub.status.idle":"2023-12-06T05:34:40.895951Z","shell.execute_reply.started":"2023-12-06T05:34:40.580816Z","shell.execute_reply":"2023-12-06T05:34:40.895073Z"},"trusted":true},"execution_count":null,"outputs":[]}]}